{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a826bf",
   "metadata": {},
   "source": [
    "# Cellstates JAX vs CPU (Colab ready)\n",
    "\n",
    "This notebook installs `cellstates`, builds a tiny synthetic dataset, and times the JAX-accelerated vs. CPU gene contribution calculations. It is meant to be run in Google Colab; just set the runtime to GPU if you want to test JAX on GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be652cd4",
   "metadata": {},
   "source": [
    "**How to use**\n",
    "\n",
    "1. Run the install cell below (set `USE_GPU=True` only if your Colab runtime has a GPU).\n",
    "2. Generate the toy dataset (<=100 cells, <=200 genes) with a fixed seed.\n",
    "3. Benchmark `gene_contribution_table` (CPU) and `gene_contribution_table_jax` (JAX) with timings and parity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install cellstates (Colab)\n",
    "# Clean any previous checkout so it doesn't shadow the installed package\n",
    "!rm -rf cellstates\n",
    "\n",
    "# 1) Make sure build tools and git are present\n",
    "!apt-get update -qq\n",
    "!apt-get install -y build-essential git\n",
    "\n",
    "# 2) Python deps that setup.py expects\n",
    "!pip install -q cython numpy scipy pandas matplotlib\n",
    "\n",
    "# 3) Clone your fork\n",
    "!git clone https://github.com/RemyNicolle/cellstates.git\n",
    "%cd cellstates\n",
    "\n",
    "# 4) Build and install via pip (handles the C extension build)\n",
    "!python -m pip install --no-build-isolation --no-cache-dir .\n",
    "\n",
    "# 5) Install JAX for the JAX benchmark (CPU by default; swap to cuda12_pip on GPU runtimes)\n",
    "# 5) Install JAX for the JAX benchmark (CPU by default; swap to cuda12_pip on GPU runtimes)\n",
    "# !pip install -q 'jax[cpu]' -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
    "# For GPU runtimes, instead run:\n",
    "# !pip install -q 'jax[cuda12_pip]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# For TPU runtimes, instead run:\n",
    "!pip install -q 'jax[tpu]' -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
    "\n",
    "# 6) Go back to the notebook root and remove the checkout so imports use site-packages\n",
    "%cd /content\n",
    "!rm -rf cellstates\n",
    "\n",
    "# 7) Verify the import resolves to the installed package\n",
    "import importlib.util, cellstates\n",
    "print(\"cellstates spec:\", importlib.util.find_spec(\"cellstates\"))\n",
    "from cellstates import Cluster, get_hierarchy_df\n",
    "print(\"Cluster import OK; module file:\", cellstates.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellstates\n",
    "from cellstates import Cluster, get_hierarchy_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c224a",
   "metadata": {},
   "source": [
    "## Generate a tiny synthetic dataset\n",
    "We keep it small (<=100 cells, <=200 genes) so it runs fast in Colab. A fixed seed ensures reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cellstates import Cluster, get_hierarchy_df\n",
    "\n",
    "SEED = 7\n",
    "GENES = 1000\n",
    "CELLS = 200\n",
    "CLUSTERS = 20\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "cells_per_cluster = np.full(CLUSTERS, CELLS // CLUSTERS)\n",
    "cells_per_cluster[: CELLS % CLUSTERS] += 1\n",
    "\n",
    "base_rates = rng.gamma(shape=1.5, scale=1.0, size=(GENES, CLUSTERS))\n",
    "cluster_scales = np.linspace(0.7, 1.3, CLUSTERS)\n",
    "cluster_rates = base_rates * cluster_scales\n",
    "\n",
    "counts_parts = []\n",
    "labels = []\n",
    "for idx, n_cells in enumerate(cells_per_cluster):\n",
    "    lam = cluster_rates[:, idx : idx + 1]\n",
    "    counts_parts.append(rng.poisson(lam, size=(GENES, n_cells)))\n",
    "    labels.append(np.full(n_cells, idx, dtype=np.int32))\n",
    "\n",
    "counts = np.concatenate(counts_parts, axis=1).astype(np.int64)\n",
    "init_clusters = np.concatenate(labels)\n",
    "\n",
    "print(f\"Counts shape: {counts.shape} | cluster sizes: {np.bincount(init_clusters)}\")\n",
    "\n",
    "clst_cpu = Cluster(counts, c=init_clusters, max_clusters=init_clusters.max() + 1, seed=SEED)\n",
    "hierarchy, delta_ll = clst_cpu.get_cluster_hierarchy()\n",
    "hierarchy_df = get_hierarchy_df(hierarchy, delta_ll)\n",
    "print(f\"Hierarchy steps: {hierarchy_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e041f3",
   "metadata": {},
   "source": [
    "## JAX hierarchy prototype (float32, TPU-friendly, chunked)\n",
    "This computes the cluster hierarchy directly in JAX from aggregated cluster counts. It operates on the current partition (clusters), not individual cells, and uses float32 to keep memory lower on TPU/GPU. The greedy search is still O(K^2), but you can chunk pair evaluations to reduce peak memory at the cost of more host/device transfers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7bbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from cellstates import get_cluster_hierarchy_jax_from_counts\n",
    "\n",
    "# Aggregate counts per cluster (shape: genes x clusters)\n",
    "counts_per_cluster = np.zeros((counts.shape[0], init_clusters.max() + 1), dtype=np.int64)\n",
    "for idx in range(counts_per_cluster.shape[1]):\n",
    "    mask = init_clusters == idx\n",
    "    if mask.any():\n",
    "        counts_per_cluster[:, idx] = counts[:, mask].sum(axis=1)\n",
    "\n",
    "lam = np.asarray(clst_cpu.dirichlet_pseudocounts, dtype=np.float32)\n",
    "\n",
    "# Device preference: TPU > GPU > CPU\n",
    "if available_jax_devices(\"tpu\"):\n",
    "    device_preference = \"tpu\"\n",
    "elif available_jax_devices(\"gpu\"):\n",
    "    device_preference = \"gpu\"\n",
    "else:\n",
    "    device_preference = \"cpu\"\n",
    "\n",
    "pair_chunk_size = 10_000  # tune down to lower peak memory\n",
    "\n",
    "merges_jax, delta_jax = get_cluster_hierarchy_jax_from_counts(\n",
    "    counts_per_cluster,\n",
    "    lam,\n",
    "    device=device_preference,\n",
    "    enable_x64=False,\n",
    "    dtype=jax.numpy.float32,\n",
    "    pair_chunk_size=pair_chunk_size,\n",
    ")\n",
    "\n",
    "print(f\"JAX hierarchy merges: {len(merges_jax)}\")\n",
    "print(\"First 5 merges:\", merges_jax[:5])\n",
    "print(\"First 5 delta_LL:\", [float(x) for x in delta_jax[:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1cefd8",
   "metadata": {},
   "source": [
    "## JAX greedy partition (multi-sweep prototype)\n",
    "Runs float32 JAX-based greedy sweeps over cells with optional candidate pruning. Shuffles cell order between sweeps. Not a full MCMC replica, but TPU-friendly and chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fa771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellstates import run_greedy_partition_jax\n",
    "\n",
    "# Tune these to trade memory vs speed/coverage\n",
    "cluster_chunk = 128\n",
    "candidate_topk = 32  # restrict proposals to top-N clusters per cell (None to scan all)\n",
    "sweeps = 3\n",
    "\n",
    "new_clusters, moves, total_delta = run_greedy_partition_jax(\n",
    "    counts,\n",
    "    init_clusters,\n",
    "    lam=np.asarray(clst_cpu.dirichlet_pseudocounts, dtype=np.float32),\n",
    "    sweeps=sweeps,\n",
    "    device=device_preference,\n",
    "    enable_x64=False,\n",
    "    dtype=jax.numpy.float32,\n",
    "    cluster_chunk=cluster_chunk,\n",
    "    candidate_topk=candidate_topk,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Sweeps: {sweeps}, total moves: {moves}, total delta_LL: {total_delta:.3f}\")\n",
    "print(\"New cluster labels (first 10):\", new_clusters[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106662e",
   "metadata": {},
   "source": [
    "## Benchmark CPU vs JAX gene contribution scores\n",
    "The first JAX call includes compilation; the second shows cached execution time. Both paths are compared for numerical parity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Optional: relax GPU preallocation to trim RAM spikes\n",
    "os.environ.setdefault(\"XLA_PYTHON_CLIENT_PREALLOCATE\", \"false\")\n",
    "os.environ.setdefault(\"XLA_PYTHON_CLIENT_MEM_FRACTION\", \"0.7\")\n",
    "os.environ.setdefault(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"platform\")\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "from cellstates import (\n",
    "    Cluster,\n",
    "    available_jax_devices,\n",
    "    gene_contribution_table,\n",
    "    gene_contribution_table_jax,\n",
    "    jax_available,\n",
    ")\n",
    "\n",
    "# Favor float64 for comparison to match CPU precision\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "\n",
    "def time_call(fn):\n",
    "    start = time.perf_counter()\n",
    "    out = fn()\n",
    "    return out, time.perf_counter() - start\n",
    "\n",
    "\n",
    "if not jax_available():\n",
    "    raise RuntimeError(\"JAX is not available; run the install cell first.\")\n",
    "\n",
    "# Prioritize TPU, then GPU, then CPU\n",
    "if available_jax_devices(\"tpu\"):\n",
    "    device_preference = \"tpu\"\n",
    "elif available_jax_devices(\"gpu\"):\n",
    "    device_preference = \"gpu\"\n",
    "else:\n",
    "    device_preference = \"cpu\"\n",
    "\n",
    "# Fresh cluster for the JAX path so we avoid any state mutations\n",
    "clst_jax = Cluster(counts, c=init_clusters, max_clusters=init_clusters.max() + 1, seed=SEED)\n",
    "\n",
    "cpu_scores, cpu_time = time_call(lambda: gene_contribution_table(clst_cpu, hierarchy_df))\n",
    "\n",
    "print(f\"JAX devices detected: {jax.devices()}\")\n",
    "print(f\"Device used for JAX helper: {device_preference}\")\n",
    "print(f\"JAX enable_x64: {jax.config.read('jax_enable_x64')}\")\n",
    "print(f\"CPU gene_contribution_table: {cpu_time:.4f} s\")\n",
    "\n",
    "jax_scores_first, jax_time_first = time_call(\n",
    "    lambda: gene_contribution_table_jax(\n",
    "        clst_jax,\n",
    "        hierarchy_df,\n",
    "        device=device_preference,\n",
    "        enable_x64=True, # Set to True to match CPU precision\n",
    "    )\n",
    ")\n",
    "\n",
    "jax_scores_cached, jax_time_cached = time_call(\n",
    "    lambda: gene_contribution_table_jax(\n",
    "        clst_jax,\n",
    "        hierarchy_df,\n",
    "        device=device_preference,\n",
    "        enable_x64=True, # Set to True to match CPU precision\n",
    "    )\n",
    ")\n",
    "\n",
    "max_diff = float(np.max(np.abs(cpu_scores - jax_scores_cached)))\n",
    "np.testing.assert_allclose(cpu_scores, jax_scores_cached, rtol=5e-4, atol=5e-2)\n",
    "\n",
    "\n",
    "print(f\"JAX gene_contribution_table_jax (compile+run): {jax_time_first:.4f} s\")\n",
    "print(f\"JAX gene_contribution_table_jax (cached run): {jax_time_cached:.4f} s\")\n",
    "print(f\"Max |CPU - JAX| difference: {max_diff:.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
